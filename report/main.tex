\documentclass{article}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{overpic}
\usepackage{amssymb}
\usepackage{url}

\usepackage[accepted]{dlai2021}

\dlaititlerunning{Stock Prediction using Neural Networks}

\begin{document}

\twocolumn[

\dlaititle{Stock Prediction using Deep Learning}

\begin{center}\today\end{center}

\begin{dlaiauthorlist}
\dlaiauthor{Nefeli Tavoulari}{}
\end{dlaiauthorlist}
\dlaicorrespondingauthor{}{tavoulari.1977701@studenti.uniroma1.it}
\vskip 0.3in
]

\printAffiliationsAndNotice{}


\section{Introduction}

\paragraph*{Topic.}
Forecasting stock prices can yield high profit to investors. There are many techniques to accomplish that, including Fundamental Analysis, Technical Analysis and Machine Learning. This paper analyzes the procedure of predicting TESLA stock prices, experimenting with different neural networks. These types of neural networks are not necessarily suitable for our goal, but they were chosen simply so that I can familiarize with all these different concepts and organize them in my brain.
\paragraph*{Challenges.}
Predicting stock prices is a challenging task, due to the many factors that cause them to go up or down (e.g. national economic and social situation). Moreover, time-series forecasting is challenging because of the data not being stationary.

\section{Methodology}
In this section I would like to say a few words about each type of neural network used in this experiment, but also about some relative concepts.
\paragraph*{Artificial Neural Networks.}
ANNs are composed of multiple nodes, which imitate biological neurons of human brain. The neurons are connected by links and they interact with each other. The nodes can take input data and perform simple operations on the data. The result of these operations is passed to other neurons. The output at each node is called its activation or node value.Each link is associated with weight. ANNs are capable of learning, which takes place by altering weight values.


\paragraph*{Multi-Layer Perceptron.}An MLP is a class of feedforward artificial neural network (ANN).It consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training.Its multiple layers and non-linear activation distinguish MLP from a linear perceptron.

\paragraph*{Convolutional Neural Network.}
CNN are distinguished from other neural networks by their superior performance with image, speech, or audio signal inputs. They have three main types of layers, which are:

Convolutional layer \\
Pooling layer \\
Fully-connected (FC) layer \\ \\
The convolutional layer is the first layer of a convolutional network. While convolutional layers can be followed by additional convolutional layers or pooling layers, the fully-connected layer is the final layer. With each layer, the CNN increases in its complexity, identifying greater portions of the image. Earlier layers focus on simple features, such as colors and edges. As the image data progresses through the layers of the CNN, it starts to recognize larger elements or shapes of the object until it finally identifies the intended object

\paragraph*{Recurrent Neural Network.}
An RNN is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.[1][2][3] This makes them applicable to tasks such as unsegmented, connected handwriting recognition[4] or speech recognition.

\paragraph*{Long Short-Term Memory.}
LSTM is an artificial recurrent neural network (RNN) architecture[1] used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections.A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.

LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications.

\paragraph*{Gated Recurrent Unit.}
GRUs are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al.The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate.GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM.GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets.

\section{Tools}
- PyTorch
- Pandas
- NumPy
- Plotly
- DVC

\section{Results}\label{sec:latex}


\paragraph*{Table.}

\begin{table}[h!]
\caption{Performance comparison.}
\label{tab:results}
\begin{center}
\begin{small}
\begin{tabular}{p{0.16\linewidth} | ccccc}

& \multirow{1}{0.1\linewidth}{MLP}& \multirow{1}{0.1\linewidth}{CNN}& \multirow{1}{0.1\linewidth}{LSTM}& \multirow{1}{0.1\linewidth}{GRU}\\
\toprule
Train RMSE & 37.585734 &  5.817905 & 10.604635 &  5.883437\\
\midrule
Test RMSE      & 66.905527 & 45.338950 & 70.779618 & 32.251542\\
\midrule
Train Time & 0.399081 &  1.096348 & 17.377151 & 10.162085 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vspace{-0.5cm}
\end{table}

\section{Conclusion}



\section{References}

\url{https://towardsdatascience.com/3-facts-about-time-series-forecasting-that-surprise-experienced-machine-learning-practitioners-69c18ee89387}

\url{https://www.tutorialspoint.com/artificial_intelligence/artificial_intelligence_neural_networks.html}

\url{https://en.wikipedia.org/wiki/Recurrent_neural_network}

\url{https://en.wikipedia.org/wiki/Long_short-term_memory}

\url{https://en.wikipedia.org/wiki/Gated_recurrent_unit}

\url{https://www.ibm.com/cloud/learn/convolutional-neural-networks}
\end{document}

